{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b549b6",
   "metadata": {},
   "source": [
    "# English-to-Urdu Machine Translation with a Transformer from Scratch\n",
    "\n",
    "This notebook implements a complete machine translation pipeline to translate text from English to Urdu. The core of this project is a Transformer model built from scratch using PyTorch, based on the architecture originally proposed by Vaswani et al. in \"Attention is All You Need.\"\n",
    "\n",
    "### Project Objectives\n",
    "\n",
    "1.  **Build a Transformer Model**: Construct the encoder-decoder architecture from the ground up.\n",
    "2.  **Train on a Parallel Corpus**: Preprocess and train the model on a curated English-Urdu dataset.\n",
    "3.  **Evaluate Performance**: Measure translation quality using the BLEU score and analyze sample translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4686f8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This section configures the environment by importing necessary libraries and setting up the device for training. Key steps include:\n",
    "\n",
    "-   **Importing Libraries**: Core libraries such as `torch`, `pandas`, and `numpy` are imported.\n",
    "-   **Device Configuration**: The code detects and selects the appropriate device (`MPS` for Apple Silicon, `CUDA` for NVIDIA GPUs, or `CPU`). A fallback to the CPU is enabled for MPS to prevent errors with unsupported operations.\n",
    "-   **Directory Creation**: Directories for saving model checkpoints and results are created to keep the project organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316d644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T10:12:18.364438Z",
     "start_time": "2025-10-26T10:12:18.351382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import json\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Fix for MPS unsupported operations - enable CPU fallback\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tokenization and NLP\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data for BLEU and METEOR\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\" MPS CPU fallback enabled for unsupported operations\")\n",
    "print(\" NLTK data downloaded (punkt, wordnet, omw)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c777b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Configure device - prioritize MPS for Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Using MPS (Metal Performance Shaders) - Apple Silicon GPU\")\n",
    "    print(f\"  PyTorch version: {torch.__version__}\")\n",
    "    print(f\"  MPS backend available: {torch.backends.mps.is_available()}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\" Using CUDA\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Using CPU\")\n",
    "\n",
    "print(f\"\\nActive device: {device}\")\n",
    "\n",
    "# Create directories for checkpoints and results\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "print(\"\\n Directory structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298829f3",
   "metadata": {},
   "source": [
    "## 2. Model and Training Configuration\n",
    "\n",
    "This section centralizes all hyperparameters and configuration settings for the model, data, and training process.\n",
    "\n",
    "-   **`config` Dictionary**: A dictionary that stores all key parameters for easy access and modification.\n",
    "-   **Data Paths**: Specifies the file paths for the English and Urdu source texts.\n",
    "-   **Model Architecture**: Defines the core parameters of the Transformer, including embedding dimensions, number of attention heads, and layer counts.\n",
    "-   **Training Hyperparameters**: Sets the learning rate, batch size, and number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "config = {\n",
    "    # Transformer architecture\n",
    "    'd_model': 512,                     # Embedding dimension for token representations\n",
    "    'nhead': 8,                         # Number of attention heads\n",
    "    'num_encoder_layers': 4,            # Encoder stack depth\n",
    "    'num_decoder_layers': 4,            # Decoder stack depth\n",
    "    'dim_feedforward': 2048,            # Feed-forward hidden size\n",
    "    'dropout': 0.3,                     # Dropout probability throughout the model\n",
    "    'max_seq_length': 128,              # Maximum sequence length supported by positional encoding\n",
    "\n",
    "    # Training parameters\n",
    "    'batch_size': 128,                  # Mini-batch size for data loaders\n",
    "    'gradient_accumulation_steps': 2,   # Steps to accumulate gradients before each optimizer step\n",
    "    'num_epochs': 100,                  # Maximum number of training epochs\n",
    "    'learning_rate': 5e-6,              # Initial learning rate prior to scheduling\n",
    "    'warmup_steps': 1000,               # Warmup steps for the Noam scheduler\n",
    "    'max_grad_norm': 1.0,               # Gradient clipping threshold\n",
    "    'early_stopping_patience': 3,       # Early-stopping patience measured in epochs\n",
    "    'weight_decay': 0.0001,             # L2 regularization 10x\n",
    "\n",
    "    # Vocabulary\n",
    "    'vocab_size_en': 10000,             # Maximum vocabulary size for English tokens\n",
    "    'vocab_size_ur': 10000,             # Maximum vocabulary size for Urdu tokens\n",
    "\n",
    "    # Special tokens\n",
    "    'pad_token': '<pad>',\n",
    "    'sos_token': '<sos>',\n",
    "    'eos_token': '<eos>',\n",
    "    'unk_token': '<unk>',\n",
    "\n",
    "    # Checkpoint settings\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'save_every_n_epochs': 20,          # Frequency (in epochs) for periodic checkpointing\n",
    "\n",
    "    # Evaluation\n",
    "    'beam_size': 5,                     # Beam width used during evaluation when beam search is enabled\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:30s}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey training utilities:\")\n",
    "print(\"   Gradient accumulation for stable updates\")\n",
    "print(\"   Early stopping to avoid overfitting\")\n",
    "print(\"   Regular checkpointing for reproducibility\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d9caa",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "This section covers loading the parallel corpus and applying initial cleaning steps.\n",
    "\n",
    "-   **Load Data**: The English and Urdu text files are loaded into a `pandas` DataFrame.\n",
    "-   **Clean Text**:\n",
    "    -   Unicode normalization is applied to standardize characters.\n",
    "    -   Leading/trailing whitespace and special characters are removed to reduce noise.\n",
    "-   **Verify**: The cleaned DataFrame is displayed to confirm the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa92b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "EN_FILE = DATA_DIR / \"english-corpus.txt\"\n",
    "UR_FILE = DATA_DIR / \"urdu-corpus.txt\"\n",
    "\n",
    "\n",
    "def load_parallel_corpus(file_path=None, en_file=None, ur_file=None, max_samples=None):\n",
    "    \"\"\"Load an English–Urdu parallel corpus from text files or a CSV.\"\"\"\n",
    "    pairs = []\n",
    "\n",
    "    if en_file and ur_file and en_file.exists() and ur_file.exists():\n",
    "        with open(en_file, \"r\", encoding=\"utf-8\") as f_en, open(ur_file, \"r\", encoding=\"utf-8\") as f_ur:\n",
    "            for en_line, ur_line in zip(f_en, f_ur):\n",
    "                en = en_line.strip()\n",
    "                ur = ur_line.strip()\n",
    "                if en and ur:\n",
    "                    pairs.append((en, ur))\n",
    "                    if max_samples and len(pairs) >= max_samples:\n",
    "                        break\n",
    "        return pairs\n",
    "\n",
    "    if file_path and file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        en_col = next((col for col in df.columns if col.lower().strip() in {\"english\", \"en\", \"source\", \"src\"}), None)\n",
    "        ur_col = next((col for col in df.columns if col.lower().strip() in {\"urdu\", \"ur\", \"target\", \"tgt\"}), None)\n",
    "        if en_col is None or ur_col is None:\n",
    "            raise ValueError(\"Could not identify English and Urdu columns in the CSV file.\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            en = str(row[en_col]).strip()\n",
    "            ur = str(row[ur_col]).strip()\n",
    "            if en and ur:\n",
    "                pairs.append((en, ur))\n",
    "                if max_samples and len(pairs) >= max_samples:\n",
    "                    break\n",
    "        return pairs\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset not found. Ensure parallel text files or a CSV are placed in the data directory.\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "parallel_data = load_parallel_corpus(en_file=EN_FILE, ur_file=UR_FILE, max_samples=None)\n",
    "print(f\"Loaded {len(parallel_data):,} parallel sentence pairs\")\n",
    "\n",
    "# Display dataset statistics\n",
    "if parallel_data:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total pairs: {len(parallel_data):,}\")\n",
    "\n",
    "    en_lengths = [len(en.split()) for en, _ in parallel_data]\n",
    "    ur_lengths = [len(ur.split()) for _, ur in parallel_data]\n",
    "\n",
    "    print(f\"\\nEnglish sentences:\")\n",
    "    print(f\"  Average length: {np.mean(en_lengths):.2f} words\")\n",
    "    print(f\"  Max length: {max(en_lengths)} words\")\n",
    "    print(f\"  Min length: {min(en_lengths)} words\")\n",
    "    print(f\"  Median length: {np.median(en_lengths):.2f} words\")\n",
    "\n",
    "    print(f\"\\nUrdu sentences:\")\n",
    "    print(f\"  Average length: {np.mean(ur_lengths):.2f} words\")\n",
    "    print(f\"  Max length: {max(ur_lengths)} words\")\n",
    "    print(f\"  Min length: {min(ur_lengths)} words\")\n",
    "    print(f\"  Median length: {np.median(ur_lengths):.2f} words\")\n",
    "\n",
    "    # Show length distribution\n",
    "    long_en = sum(1 for l in en_lengths if l > config['max_seq_length'])\n",
    "    long_ur = sum(1 for l in ur_lengths if l > config['max_seq_length'])\n",
    "    print(f\"\\nLength distribution:\")\n",
    "    print(f\"  Sentences > {config['max_seq_length']} words (EN): {long_en} ({long_en/len(en_lengths)*100:.1f}%)\")\n",
    "    print(f\"  Sentences > {config['max_seq_length']} words (UR): {long_ur} ({long_ur/len(ur_lengths)*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nSample pairs:\")\n",
    "    for i in range(min(5, len(parallel_data))):\n",
    "        en, ur = parallel_data[i]\n",
    "        print(f\"  Pair {i+1} EN: {en}\")\n",
    "        print(f\"           UR: {ur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_urdu(text):\n",
    "    \"\"\"\n",
    "    Normalize Urdu text for better processing.\n",
    "    \n",
    "    Args:\n",
    "        text: Urdu text string\n",
    "    \n",
    "    Returns:\n",
    "        Normalized Urdu text\n",
    "    \"\"\"\n",
    "    # Normalize Unicode characters (NFKC normalization)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove Arabic diacritics (harakat) which can cause tokenization issues\n",
    "    # Range: U+064B to U+065F and U+0670\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, is_urdu=False):\n",
    "    \"\"\"\n",
    "    Normalize and clean text for English or Urdu.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        is_urdu: Whether the text is in Urdu\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if is_urdu:\n",
    "        # Use specialized Urdu normalization\n",
    "        return normalize_urdu(text)\n",
    "    else:\n",
    "        # English normalization\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^a-z0-9\\s.,!?\\'-]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_tokenize(text, is_urdu=False):\n",
    "    \"\"\"\n",
    "    Simple word-level tokenization.\n",
    "    For production, use more sophisticated tokenizers like sentencepiece or transformers.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        is_urdu: Whether the text is Urdu\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    text = normalize_text(text, is_urdu)\n",
    "    \n",
    "    # Split on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\S+', text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test tokenization\n",
    "print(\"Testing tokenization:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_en = \"Hello, how are you today?\"\n",
    "test_ur = \"ہیلو، آپ آج کیسے ہیں؟\"\n",
    "\n",
    "en_tokens = simple_tokenize(test_en, is_urdu=False)\n",
    "ur_tokens = simple_tokenize(test_ur, is_urdu=True)\n",
    "\n",
    "print(f\"English: {test_en}\")\n",
    "print(f\"Tokens:  {en_tokens}\")\n",
    "print(f\"Count:   {len(en_tokens)}\")\n",
    "\n",
    "print(f\"\\nUrdu: {test_ur}\")\n",
    "print(f\"Tokens: {ur_tokens}\")\n",
    "print(f\"Count:  {len(ur_tokens)}\")\n",
    "\n",
    "# Test normalization\n",
    "print(f\"\\nUrdu normalization test:\")\n",
    "test_ur_with_diacritics = \"مَرحَبا\"  # With diacritics\n",
    "normalized = normalize_urdu(test_ur_with_diacritics)\n",
    "print(f\"  Original:   {test_ur_with_diacritics}\")\n",
    "print(f\"  Normalized: {normalized}\")\n",
    "print(f\"  Length before: {len(test_ur_with_diacritics)}, after: {len(normalized)}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" Tokenization and normalization ready!\")\n",
    "print(\"  • Urdu text undergoes Unicode normalization (NFKC)\")\n",
    "print(\"  • Arabic diacritics are removed for consistency\")\n",
    "print(\"  • English text is lowercased and cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d19b9",
   "metadata": {},
   "source": [
    "## 4. Vocabulary and Tokenization\n",
    "\n",
    "This section defines the `Vocabulary` class, which is responsible for converting text into numerical tokens that the model can process.\n",
    "\n",
    "-   **Build Vocabulary**: The class is initialized with raw text sentences and builds a mapping from words to integer indices. Special tokens like `<PAD>`, `<SOS>`, `<EOS>`, and `<UNK>` are included.\n",
    "-   **Tokenize and Detokenize**: Methods are provided to convert sentences into sequences of tokens (`numericalize`) and back into human-readable text (`denumericalize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Build vocabulary from corpus with special tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=10000, min_freq=2):\n",
    "        self.max_size = max_size\n",
    "        self.min_freq = min_freq\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        self.token_freq = Counter()\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.pad_token = config['pad_token']\n",
    "        self.sos_token = config['sos_token']\n",
    "        self.eos_token = config['eos_token']\n",
    "        self.unk_token = config['unk_token']\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
    "        for token in special_tokens:\n",
    "            self.add_token(token)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token to vocabulary.\"\"\"\n",
    "        if token not in self.token2idx:\n",
    "            idx = len(self.token2idx)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "    \n",
    "    def build_vocab(self, sentences, is_urdu=False):\n",
    "        \"\"\"Build vocabulary from list of sentences.\"\"\"\n",
    "        # Count token frequencies\n",
    "        for sentence in sentences:\n",
    "            tokens = simple_tokenize(sentence, is_urdu)\n",
    "            self.token_freq.update(tokens)\n",
    "        \n",
    "        # Add most frequent tokens (excluding special tokens)\n",
    "        most_common = self.token_freq.most_common(self.max_size - 4)  # -4 for special tokens\n",
    "        \n",
    "        for token, freq in most_common:\n",
    "            if freq >= self.min_freq and token not in self.token2idx:\n",
    "                self.add_token(token)\n",
    "        \n",
    "        print(f\"Built vocabulary with {len(self.token2idx)} tokens\")\n",
    "        print(f\"  Most common: {self.token_freq.most_common(10)}\")\n",
    "    \n",
    "    def encode(self, text, is_urdu=False):\n",
    "        \"\"\"Convert text to list of token indices.\"\"\"\n",
    "        tokens = simple_tokenize(text, is_urdu)\n",
    "        indices = [self.token2idx.get(token, self.token2idx[self.unk_token]) for token in tokens]\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of indices back to text.\"\"\"\n",
    "        tokens = [self.idx2token.get(idx, self.unk_token) for idx in indices]\n",
    "        # Remove special tokens for display\n",
    "        tokens = [t for t in tokens if t not in [self.pad_token, self.sos_token, self.eos_token]]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2idx)\n",
    "\n",
    "# Build vocabularies\n",
    "print(\"Building vocabularies...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract English and Urdu sentences\n",
    "english_sentences = [en for en, _ in parallel_data]\n",
    "urdu_sentences = [ur for _, ur in parallel_data]\n",
    "\n",
    "# Create vocabularies\n",
    "vocab_en = Vocabulary(max_size=config['vocab_size_en'], min_freq=1)\n",
    "vocab_ur = Vocabulary(max_size=config['vocab_size_ur'], min_freq=1)\n",
    "\n",
    "vocab_en.build_vocab(english_sentences, is_urdu=False)\n",
    "print()\n",
    "vocab_ur.build_vocab(urdu_sentences, is_urdu=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VOCABULARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"English vocabulary size: {len(vocab_en)}\")\n",
    "print(f\"Urdu vocabulary size: {len(vocab_ur)}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_sentence = english_sentences[0] if english_sentences else \"hello world\"\n",
    "encoded = vocab_en.encode(test_sentence, is_urdu=False)\n",
    "decoded = vocab_en.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest encoding/decoding:\")\n",
    "print(f\"  Original: {test_sentence}\")\n",
    "print(f\"  Encoded:  {encoded[:20]}...\")  # Show first 20 indices\n",
    "print(f\"  Decoded:  {decoded}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ac752",
   "metadata": {},
   "source": [
    "## 5. Custom `Dataset` and `DataLoader`\n",
    "\n",
    "This section prepares the data for training by creating a custom `Dataset` and wrapping it in a `DataLoader`.\n",
    "\n",
    "-   **`TranslationDataset`**: A custom class that holds tokenized English and Urdu sentence pairs.\n",
    "-   **Collation Function**: The `collate_fn` pads sequences to the same length within each batch, ensuring uniform tensor shapes.\n",
    "-   **`DataLoader`**: A `DataLoader` is created to efficiently batch and shuffle the data, making it ready for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset for English-Urdu parallel sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, pairs, vocab_src, vocab_tgt, max_length=100):\n",
    "        self.pairs = pairs\n",
    "        self.vocab_src = vocab_src\n",
    "        self.vocab_tgt = vocab_tgt\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text, tgt_text = self.pairs[idx]\n",
    "\n",
    "        # Encode source and target\n",
    "        src_indices = self.vocab_src.encode(src_text, is_urdu=False)\n",
    "        tgt_indices = self.vocab_tgt.encode(tgt_text, is_urdu=True)\n",
    "\n",
    "        # Validate that sequences are not empty\n",
    "        if len(src_indices) == 0:\n",
    "            src_indices = [self.vocab_src.token2idx[self.vocab_src.unk_token]]\n",
    "        if len(tgt_indices) == 0:\n",
    "            tgt_indices = [self.vocab_tgt.token2idx[self.vocab_tgt.unk_token]]\n",
    "\n",
    "        # Truncate if too long\n",
    "        src_indices = src_indices[:self.max_length]\n",
    "        tgt_indices = tgt_indices[:self.max_length]\n",
    "\n",
    "        # Add SOS and EOS tokens to target\n",
    "        sos_idx = self.vocab_tgt.token2idx[self.vocab_tgt.sos_token]\n",
    "        eos_idx = self.vocab_tgt.token2idx[self.vocab_tgt.eos_token]\n",
    "\n",
    "        tgt_indices = [sos_idx] + tgt_indices + [eos_idx]\n",
    "\n",
    "        return {\n",
    "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_indices, dtype=torch.long),\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle variable-length sequences with error handling.\n",
    "\n",
    "    Args:\n",
    "        batch: List of samples from dataset\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with padded batches\n",
    "    \"\"\"\n",
    "    # Filter out None items and validate\n",
    "    valid_batch = []\n",
    "    for item in batch:\n",
    "        if item is None:\n",
    "            print(\"  WARNING: Skipping None item in batch\")\n",
    "            continue\n",
    "        if 'src' not in item or 'tgt' not in item:\n",
    "            print(\"  WARNING: Skipping invalid item (missing src or tgt)\")\n",
    "            continue\n",
    "        if len(item['src']) == 0 or len(item['tgt']) == 0:\n",
    "            print(\"  WARNING: Skipping item with empty sequence\")\n",
    "            continue\n",
    "        valid_batch.append(item)\n",
    "\n",
    "    # Handle empty batch\n",
    "    if len(valid_batch) == 0:\n",
    "        raise ValueError(\"Empty batch after filtering invalid items!\")\n",
    "\n",
    "    # Extract sequences\n",
    "    src_batch = [item['src'] for item in valid_batch]\n",
    "    tgt_batch = [item['tgt'] for item in valid_batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    pad_idx = vocab_en.token2idx[vocab_en.pad_token]\n",
    "\n",
    "    try:\n",
    "        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)\n",
    "        tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=pad_idx)\n",
    "    except Exception as e:\n",
    "        print(f\" Error in padding: {e}\")\n",
    "        print(f\"   Batch size: {len(valid_batch)}\")\n",
    "        print(f\"   Source lengths: {[len(s) for s in src_batch]}\")\n",
    "        print(f\"   Target lengths: {[len(t) for t in tgt_batch]}\")\n",
    "        raise\n",
    "\n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded,\n",
    "        'src_texts': [item['src_text'] for item in valid_batch],\n",
    "        'tgt_texts': [item['tgt_text'] for item in valid_batch]\n",
    "    }\n",
    "\n",
    "\n",
    "# Split data into train/val/test\n",
    "train_size = int(0.8 * len(parallel_data))\n",
    "val_size = int(0.1 * len(parallel_data))\n",
    "test_size = len(parallel_data) - train_size - val_size\n",
    "\n",
    "# Shuffle data\n",
    "random.shuffle(parallel_data)\n",
    "\n",
    "train_pairs = parallel_data[:train_size]\n",
    "val_pairs = parallel_data[train_size:train_size + val_size]\n",
    "test_pairs = parallel_data[train_size + val_size:]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total pairs:    {len(parallel_data):,}\")\n",
    "print(f\"Training:       {len(train_pairs):,} ({len(train_pairs)/len(parallel_data)*100:.1f}%)\")\n",
    "print(f\"Validation:     {len(val_pairs):,} ({len(val_pairs)/len(parallel_data)*100:.1f}%)\")\n",
    "print(f\"Test:           {len(test_pairs):,} ({len(test_pairs)/len(parallel_data)*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(train_pairs, vocab_en, vocab_ur, config['max_seq_length'])\n",
    "val_dataset = TranslationDataset(val_pairs, vocab_en, vocab_ur, config['max_seq_length'])\n",
    "test_dataset = TranslationDataset(test_pairs, vocab_en, vocab_ur, config['max_seq_length'])\n",
    "\n",
    "# Create dataloaders with error handling\n",
    "try:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,          # Single-worker loading for cross-platform consistency\n",
    "        pin_memory=False        # Disabled to preserve compatibility with CPU/MPS execution\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Created DataLoaders with batch size {config['batch_size']}\")\n",
    "    print(f\"  Training batches:   {len(train_loader):,}\")\n",
    "    print(f\"  Validation batches: {len(val_loader):,}\")\n",
    "    print(f\"  Test batches:       {len(test_loader):,}\")\n",
    "\n",
    "    # Test a batch to ensure everything works\n",
    "    print(f\"\\nTesting batch loading...\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\" Sample batch shapes:\")\n",
    "    print(f\"  Source: {sample_batch['src'].shape}\")\n",
    "    print(f\"  Target: {sample_batch['tgt'].shape}\")\n",
    "    print(f\"  Batch loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error creating DataLoaders: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589d888",
   "metadata": {},
   "source": [
    "## 6. Transformer Model Architecture\n",
    "\n",
    "This section provides a detailed breakdown of the Transformer model, implemented from scratch. The architecture is composed of several key components that work together to process sequential data, capture contextual relationships, and generate translations.\n",
    "\n",
    "### 6.1. Core Components\n",
    "\n",
    "#### Multi-Head Attention (`MultiHeadAttention`)\n",
    "The fundamental building block of the Transformer. This mechanism allows the model to weigh the significance of different words when producing a representation of a sequence. Instead of a single attention function, it runs multiple attention mechanisms in parallel (\"heads\"), allowing the model to jointly attend to information from different representational subspaces.\n",
    "\n",
    "#### Position-wise Feed-Forward Network (`PositionwiseFeedForward`)\n",
    "A fully connected feed-forward network that is applied to each position separately and identically. It consists of two linear transformations with a ReLU activation in between, enabling the model to learn more complex transformations.\n",
    "\n",
    "#### Positional Encoding (`PositionalEncoding`)\n",
    "Since the Transformer contains no recurrence or convolution, it relies on positional encodings to inject information about the relative or absolute position of tokens in the sequence. These encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for Transformer as described in the paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerTranslator(nn.Module):\n",
    "    \"\"\"Transformer model for machine translation.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=512,\n",
    "                 nhead=8,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 max_seq_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier uniform initialization.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generate mask for decoder to prevent attending to future tokens.\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def create_padding_mask(self, seq, pad_idx):\n",
    "        \"\"\"Create mask for padding tokens.\"\"\"\n",
    "        return (seq == pad_idx)\n",
    "    \n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequences (batch_size, src_seq_len)\n",
    "            tgt: Target sequences (batch_size, tgt_seq_len)\n",
    "            src_padding_mask: Padding mask for source (batch_size, src_seq_len)\n",
    "            tgt_padding_mask: Padding mask for target (batch_size, tgt_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Generate target mask (causal mask)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        \n",
    "        # Forward pass through transformer\n",
    "        output = self.transformer(\n",
    "            src_emb, \n",
    "            tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerTranslator(\n",
    "    src_vocab_size=len(vocab_en),\n",
    "    tgt_vocab_size=len(vocab_ur),\n",
    "    d_model=config['d_model'],\n",
    "    nhead=config['nhead'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    dim_feedforward=config['dim_feedforward'],\n",
    "    dropout=config['dropout'],\n",
    "    max_seq_length=config['max_seq_length']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(model)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc5c2b",
   "metadata": {},
   "source": [
    "### 6.4. Encoder Layer\n",
    "\n",
    "-   **`EncoderLayer`**: A single layer of the encoder stack. It consists of a multi-head self-attention mechanism followed by a position-wise feed-forward network. Residual connections and layer normalization are applied after each sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup as described in 'Attention is All You Need'.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate.\"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = self._get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "    \n",
    "    def _get_lr(self):\n",
    "        \"\"\"Calculate learning rate based on step number.\"\"\"\n",
    "        step = max(self.step_num, 1)\n",
    "        return (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "\n",
    "# Loss function (ignore padding tokens)\n",
    "pad_idx = vocab_en.token2idx[vocab_en.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=config['learning_rate'], \n",
    "    betas=(0.9, 0.98), \n",
    "    eps=1e-9,\n",
    "    weight_decay=config.get('weight_decay', 0.0001)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = NoamScheduler(optimizer, config['d_model'], config['warmup_steps'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Initial learning rate: {config['learning_rate']}\")\n",
    "print(f\"Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"Gradient clipping: {config['max_grad_norm']}\")\n",
    "print(f\"Loss function: CrossEntropyLoss with label smoothing (0.1)\")\n",
    "print(f\"Padding index (ignored): {pad_idx}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814e487",
   "metadata": {},
   "source": [
    "### 6.5. Encoder\n",
    "\n",
    "-   **`Encoder`**: The full encoder, composed of a stack of `EncoderLayer` instances. It processes the input sequence and generates a context-rich representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with comprehensive state.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save\n",
    "        optimizer: The optimizer state\n",
    "        scheduler: The scheduler state\n",
    "        epoch: Current epoch number\n",
    "        train_loss: Training loss\n",
    "        val_loss: Validation loss\n",
    "        filepath: Path to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_step': scheduler.step_num,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'config': config,\n",
    "        'vocab_en_size': len(vocab_en),\n",
    "        'vocab_ur_size': len(vocab_ur),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"   Checkpoint saved: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None, strict=True):\n",
    "    \"\"\"\n",
    "    Load model checkpoint with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to checkpoint file\n",
    "        model: Model to load weights into\n",
    "        optimizer: Optimizer to load state into (optional)\n",
    "        scheduler: Scheduler to load state into (optional)\n",
    "        strict: Whether to strictly enforce state dict keys match\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with epoch, train_loss, val_loss, or None if failed\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\" Checkpoint file not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        print(f\"Loading checkpoint from: {filepath}\")\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        \n",
    "        # Validate checkpoint structure\n",
    "        required_keys = ['model_state_dict', 'epoch']\n",
    "        missing_keys = [key for key in required_keys if key not in checkpoint]\n",
    "        \n",
    "        if missing_keys:\n",
    "            print(f\"  WARNING: Checkpoint missing keys: {missing_keys}\")\n",
    "            if strict:\n",
    "                raise ValueError(f\"Invalid checkpoint structure. Missing: {missing_keys}\")\n",
    "            return None\n",
    "        \n",
    "        # Load model state\n",
    "        try:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"   Model state loaded\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"    WARNING: Model state dict mismatch: {e}\")\n",
    "            if strict:\n",
    "                raise\n",
    "            # Try loading with strict=False\n",
    "            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "            print(f\"   Model state loaded (non-strict)\")\n",
    "        \n",
    "        # Load optimizer state (optional)\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                print(f\"   Optimizer state loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"    WARNING: Could not load optimizer state: {e}\")\n",
    "        \n",
    "        # Load scheduler state (optional)\n",
    "        if scheduler and 'scheduler_step' in checkpoint:\n",
    "            try:\n",
    "                scheduler.step_num = checkpoint['scheduler_step']\n",
    "                print(f\"   Scheduler state loaded (step: {checkpoint['scheduler_step']})\")\n",
    "            except Exception as e:\n",
    "                print(f\"    WARNING: Could not load scheduler state: {e}\")\n",
    "        \n",
    "        # Print checkpoint info\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CHECKPOINT INFO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "        if 'train_loss' in checkpoint:\n",
    "            print(f\"  Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "        if 'vocab_en_size' in checkpoint:\n",
    "            print(f\"  English Vocab Size: {checkpoint['vocab_en_size']:,}\")\n",
    "        if 'vocab_ur_size' in checkpoint:\n",
    "            print(f\"  Urdu Vocab Size: {checkpoint['vocab_ur_size']:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'epoch': checkpoint['epoch'],\n",
    "            'train_loss': checkpoint.get('train_loss', None),\n",
    "            'val_loss': checkpoint.get('val_loss', None)\n",
    "        }\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Checkpoint file not found: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading checkpoint: {e}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        if strict:\n",
    "            raise\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(\" Checkpoint management functions defined\")\n",
    "print(\"  • save_checkpoint(): Saves model, optimizer, scheduler state\")\n",
    "print(\"  • load_checkpoint(): Loads with comprehensive error handling\")\n",
    "print(\"  • Validates checkpoint structure before loading\")\n",
    "print(\"  • Graceful fallback for missing or corrupt checkpoints\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b08e9",
   "metadata": {},
   "source": [
    "### 6.6. Decoder Layer\n",
    "\n",
    "-   **`DecoderLayer`**: A single layer of the decoder stack. It includes two multi-head attention mechanisms: one for self-attention on the target sequence and another for cross-attention on the encoder's output. It also has a position-wise feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss stops improving.\n",
    "    \n",
    "    Args:\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "        min_delta: Minimum change to qualify as improvement\n",
    "        verbose: Whether to print messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0.0001, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, val_loss, epoch):\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "            epoch: Current epoch number\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if training should stop\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            if self.verbose:\n",
    "                print(f\"   Initial validation loss: {val_loss:.4f}\")\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement\n",
    "            if self.verbose:\n",
    "                improvement = self.best_loss - val_loss\n",
    "                print(f\"   Validation loss improved by {improvement:.4f} ({self.best_loss:.4f} → {val_loss:.4f})\")\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"    No improvement for {self.counter}/{self.patience} epochs\")\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\" EARLY STOPPING TRIGGERED\")\n",
    "                    print(f\"{'='*60}\")\n",
    "                    print(f\"No improvement for {self.patience} consecutive epochs\")\n",
    "                    print(f\"Best validation loss: {self.best_loss:.4f} (Epoch {self.best_epoch})\")\n",
    "                    print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the early stopping state.\"\"\"\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "\n",
    "def print_memory_stats(device):\n",
    "    \"\"\"\n",
    "    Print memory statistics for the current device.\n",
    "    \n",
    "    Args:\n",
    "        device: torch.device object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if device.type == 'mps':\n",
    "            # MPS (Metal Performance Shaders) memory tracking\n",
    "            if hasattr(torch.mps, 'current_allocated_memory'):\n",
    "                allocated = torch.mps.current_allocated_memory() / 1024**3  # Convert to GB\n",
    "                print(f\"   MPS Memory Allocated: {allocated:.2f} GB\")\n",
    "            else:\n",
    "                print(f\"   MPS memory tracking not available in this PyTorch version\")\n",
    "        elif device.type == 'cuda':\n",
    "            # CUDA memory tracking\n",
    "            allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "            print(f\"   GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        else:\n",
    "            # CPU - no specific memory tracking needed\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        # Gracefully handle if memory stats are unavailable\n",
    "        if device.type != 'cpu':\n",
    "            print(f\"   Memory stats unavailable: {e}\")\n",
    "\n",
    "\n",
    "def clear_memory_cache(device):\n",
    "    \"\"\"\n",
    "    Clear memory cache for the device.\n",
    "    \n",
    "    Args:\n",
    "        device: torch.device object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if device.type == 'mps':\n",
    "            if hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Silently fail if cache clearing not available\n",
    "\n",
    "\n",
    "# Test early stopping\n",
    "print(\"=\" * 60)\n",
    "print(\"EARLY STOPPING & MEMORY MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test early stopping logic\n",
    "early_stopping = EarlyStopping(patience=3, verbose=False)\n",
    "test_losses = [0.5, 0.48, 0.47, 0.49, 0.50, 0.51, 0.52]\n",
    "\n",
    "print(\"\\nTesting Early Stopping with patience=3:\")\n",
    "print(\"Loss sequence: [0.5, 0.48, 0.47, 0.49, 0.50, 0.51, 0.52]\")\n",
    "print()\n",
    "\n",
    "for epoch, loss in enumerate(test_losses, 1):\n",
    "    stop = early_stopping(loss, epoch)\n",
    "    status = \"STOP\" if stop else \"continue\"\n",
    "    print(f\"  Epoch {epoch}: loss={loss:.2f}, counter={early_stopping.counter} → {status}\")\n",
    "    if stop:\n",
    "        print(f\"  → Would stop at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n Early stopping implemented with patience={config['early_stopping_patience']}\")\n",
    "\n",
    "# Test memory monitoring\n",
    "print(f\"\\nTesting memory monitoring on device: {device}\")\n",
    "print_memory_stats(device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" Early stopping and memory monitoring ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30795a7",
   "metadata": {},
   "source": [
    "### 6.7. Decoder\n",
    "\n",
    "-   **`Decoder`**: The full decoder, composed of a stack of `DecoderLayer` instances. It generates the translated sequence token by token, attending to the encoder's output at each step.\n",
    "\n",
    "### 6.8. Assembling the Transformer\n",
    "\n",
    "-   **`Transformer`**: The final model that combines the `Encoder` and `Decoder`. It takes the source and target sequences as input and produces the final translation output. It also handles the creation of masks to prevent the model from attending to future tokens in the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea53762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, pad_idx, \n",
    "                gradient_accumulation_steps=1):\n",
    "    \"\"\"\n",
    "    Train for one epoch with gradient accumulation support.\n",
    "    \n",
    "    Args:\n",
    "        model: The Transformer model\n",
    "        dataloader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on\n",
    "        pad_idx: Padding token index\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    optimizer.zero_grad()  # Zero gradients at the start\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        \n",
    "        # Create padding masks\n",
    "        src_padding_mask = (src == pad_idx)\n",
    "        tgt_padding_mask = (tgt == pad_idx)\n",
    "        \n",
    "        # Target input and output (shifted by one)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_padding_mask_input = tgt_padding_mask[:, :-1]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input, src_padding_mask, tgt_padding_mask_input)\n",
    "        \n",
    "        # Calculate loss\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Normalize loss by accumulation steps\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        # Update weights every gradient_accumulation_steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            current_lr = scheduler.step()\n",
    "            \n",
    "            # Zero gradients for next accumulation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Track total loss (denormalize)\n",
    "            total_loss += accumulated_loss * gradient_accumulation_steps\n",
    "            \n",
    "            # Update progress bar\n",
    "            effective_batch = config['batch_size'] * gradient_accumulation_steps\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': accumulated_loss * gradient_accumulation_steps,\n",
    "                'lr': f'{current_lr:.2e}',\n",
    "                'eff_bs': effective_batch\n",
    "            })\n",
    "            \n",
    "            accumulated_loss = 0\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, pad_idx):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The Transformer model\n",
    "        dataloader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Device to run on\n",
    "        pad_idx: Padding token index\n",
    "    \n",
    "    Returns:\n",
    "        Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "            \n",
    "            # Create padding masks\n",
    "            src_padding_mask = (src == pad_idx)\n",
    "            tgt_padding_mask = (tgt == pad_idx)\n",
    "            \n",
    "            # Target input and output\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            tgt_padding_mask_input = tgt_padding_mask[:, :-1]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input, src_padding_mask, tgt_padding_mask_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs, device, pad_idx, checkpoint_dir, early_stopping=None,\n",
    "                gradient_accumulation_steps=1):\n",
    "    \"\"\"\n",
    "    Full training loop with early stopping, memory monitoring, and progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The Transformer model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Maximum number of epochs to train\n",
    "        device: Device to train on\n",
    "        pad_idx: Padding token index\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        early_stopping: EarlyStopping instance (optional)\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, val_losses, stopped_early)\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {config['batch_size']}\")\n",
    "    print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"Effective batch size: {config['batch_size'] * gradient_accumulation_steps}\")\n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "    \n",
    "    # Estimate training time\n",
    "    print(f\"\\nEstimated time per epoch: ~{len(train_loader) * 0.5 / 60:.1f}-{len(train_loader) * 1.0 / 60:.1f} minutes\")\n",
    "    print(f\"Estimated total time: ~{num_epochs * len(train_loader) * 0.5 / 3600:.1f}-{num_epochs * len(train_loader) * 1.0 / 3600:.1f} hours\")\n",
    "    print(f\"(Actual time depends on hardware and batch size)\")\n",
    "    \n",
    "    if early_stopping:\n",
    "        print(f\"\\nEarly stopping enabled: patience={early_stopping.patience}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stopped_early = False\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, \n",
    "                                device, pad_idx, gradient_accumulation_steps)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, device, pad_idx)\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        elapsed_total = time.time() - start_time_total\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.1f}s ({epoch_time/60:.1f}m)\")\n",
    "        print(f\"  Total Time: {elapsed_total/60:.1f}m ({elapsed_total/3600:.2f}h)\")\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        if epoch < num_epochs:\n",
    "            avg_epoch_time = elapsed_total / epoch\n",
    "            remaining_epochs = num_epochs - epoch\n",
    "            eta = avg_epoch_time * remaining_epochs\n",
    "            print(f\"  ETA: ~{eta/60:.1f}m ({eta/3600:.2f}h)\")\n",
    "        \n",
    "        # Memory stats\n",
    "        print_memory_stats(device)\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if epoch % 5 == 0:\n",
    "            clear_memory_cache(device)\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if epoch % config['save_every_n_epochs'] == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_simple_{epoch:03d}.pt\")\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(checkpoint_dir, \"best_model_simple.pt\")\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, best_model_path)\n",
    "            print(f\"   New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            if early_stopping(val_loss, epoch):\n",
    "                stopped_early = True\n",
    "                print(f\"\\nStopping early at epoch {epoch}\")\n",
    "                print(f\"Best validation loss: {early_stopping.best_loss:.4f} (Epoch {early_stopping.best_epoch})\")\n",
    "                break\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total epochs: {len(train_losses)}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Total training time: {total_time/60:.1f}m ({total_time/3600:.2f}h)\")\n",
    "    print(f\"Average time per epoch: {total_time/len(train_losses):.1f}s\")\n",
    "    print(f\"Stopped early: {stopped_early}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\" train_epoch(): Training with gradient accumulation\")\n",
    "print(\" validate(): Validation with progress tracking\")\n",
    "print(\" train_model(): Full training loop with:\")\n",
    "print(\"  • Early stopping support\")\n",
    "print(\"  • Memory monitoring\")\n",
    "print(\"  • Progress tracking with ETA\")\n",
    "print(\"  • Automatic checkpoint saving\")\n",
    "print(\"  • Learning rate display\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be002d9",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "This section initializes the Transformer model with the specified hyperparameters and moves it to the selected device.\n",
    "\n",
    "-   **Instantiate Model**: The `Transformer` class is instantiated with parameters from the `config` dictionary.\n",
    "-   **Move to Device**: The model is moved to the GPU (`mps` or `cuda`) or CPU for training.\n",
    "-   **Initialize Weights**: Model weights are initialized with a uniform distribution to ensure stable training from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config['early_stopping_patience'],\n",
    "    min_delta=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Check for existing checkpoint and offer to resume\n",
    "checkpoint_path = os.path.join(config['checkpoint_dir'], 'best_model_simple.pt')\n",
    "start_epoch = 1\n",
    "resume_training = False\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHECKPOINT FOUND\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "    print(f\"\\nOptions:\")\n",
    "    print(f\"  1. Resume training from checkpoint\")\n",
    "    print(f\"  2. Start fresh training (will backup existing checkpoint)\")\n",
    "    print(f\"  3. Skip training and use existing model for evaluation\")\n",
    "    \n",
    "    # For automated execution, check modification time\n",
    "    checkpoint_time = os.path.getmtime(checkpoint_path)\n",
    "    age_hours = (time.time() - checkpoint_time) / 3600\n",
    "    \n",
    "    print(f\"\\nCheckpoint age: {age_hours:.1f} hours\")\n",
    "    \n",
    "    # Auto-decide based on checkpoint age and whether it looks complete\n",
    "    try:\n",
    "        ckpt_info = load_checkpoint(checkpoint_path, model, strict=False)\n",
    "        if ckpt_info and ckpt_info['epoch'] >= config['num_epochs'] * 0.8:\n",
    "            print(f\"\\n Checkpoint appears complete (epoch {ckpt_info['epoch']}/{config['num_epochs']})\")\n",
    "            print(f\"  Skipping training and using this model for evaluation.\")\n",
    "            resume_training = False\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            skip_training = True\n",
    "        else:\n",
    "            print(f\"\\n  Checkpoint incomplete (epoch {ckpt_info['epoch'] if ckpt_info else 0}/{config['num_epochs']})\")\n",
    "            print(f\"  Will resume training...\")\n",
    "            resume_training = True\n",
    "            skip_training = False\n",
    "            if ckpt_info:\n",
    "                start_epoch = ckpt_info['epoch'] + 1\n",
    "    except:\n",
    "        print(f\"\\n  Could not load checkpoint. Starting fresh training...\")\n",
    "        skip_training = False\n",
    "        resume_training = False\n",
    "else:\n",
    "    print(f\"\\nNo existing checkpoint found. Starting fresh training...\")\n",
    "    skip_training = False\n",
    "    resume_training = False\n",
    "\n",
    "# Execute training if needed\n",
    "if not skip_training:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"INITIATING TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    if resume_training:\n",
    "        load_checkpoint(checkpoint_path, model, optimizer, scheduler, strict=False)\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    # Run training\n",
    "    train_losses, val_losses, stopped_early = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        device=device,\n",
    "        pad_idx=pad_idx,\n",
    "        checkpoint_dir=config['checkpoint_dir'],\n",
    "        early_stopping=early_stopping,\n",
    "        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 1)\n",
    "    )\n",
    "    \n",
    "    # Save training history\n",
    "    training_history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'stopped_early': stopped_early,\n",
    "        'total_epochs': len(train_losses),\n",
    "        'best_val_loss': min(val_losses) if val_losses else None,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    history_path = os.path.join('results', 'training_history.json')\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    with open(history_path, 'w') as f:\n",
    "        # Convert lists to serializable format\n",
    "        serializable_history = {\n",
    "            k: (v if not isinstance(v, list) else v) \n",
    "            for k, v in training_history.items()\n",
    "        }\n",
    "        json.dump(serializable_history, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n Training history saved to {history_path}\")\n",
    "else:\n",
    "    print(f\"\\n Skipping training - using existing model\")\n",
    "    # Try to load training history if it exists\n",
    "    history_path = os.path.join('results', 'training_history.json')\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "            train_losses = training_history.get('train_losses', [])\n",
    "            val_losses = training_history.get('val_losses', [])\n",
    "        print(f\" Loaded training history from {history_path}\")\n",
    "    else:\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING PHASE COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c06e4f",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "This section defines the training and evaluation loops.\n",
    "\n",
    "-   **`train_fn`**: A function that performs a single epoch of training. It iterates through the `DataLoader`, computes the model's output, calculates the loss, and updates the model's weights using backpropagation.\n",
    "-   **`eval_fn`**: A function that evaluates the model on the validation set. It computes the loss without performing backpropagation to monitor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, vocab_src, vocab_tgt, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        sentence: Source sentence (string)\n",
    "        vocab_src: Source vocabulary\n",
    "        vocab_tgt: Target vocabulary\n",
    "        device: Device to run on\n",
    "        max_length: Maximum translation length\n",
    "    \n",
    "    Returns:\n",
    "        Translated sentence (string)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source sentence\n",
    "        src_indices = vocab_src.encode(sentence, is_urdu=False)\n",
    "        src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get special token indices\n",
    "        sos_idx = vocab_tgt.token2idx[vocab_tgt.sos_token]\n",
    "        eos_idx = vocab_tgt.token2idx[vocab_tgt.eos_token]\n",
    "        pad_idx = vocab_tgt.token2idx[vocab_tgt.pad_token]\n",
    "        \n",
    "        # Start with SOS token\n",
    "        tgt_indices = [sos_idx]\n",
    "        \n",
    "        # Generate translation token by token\n",
    "        for _ in range(max_length):\n",
    "            tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Create padding masks\n",
    "            src_padding_mask = (src_tensor == pad_idx)\n",
    "            tgt_padding_mask = (tgt_tensor == pad_idx)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src_tensor, tgt_tensor, src_padding_mask, tgt_padding_mask)\n",
    "            \n",
    "            # Get prediction for last token\n",
    "            next_token_logits = output[0, -1, :]\n",
    "            next_token = next_token_logits.argmax().item()\n",
    "            \n",
    "            # Add to sequence\n",
    "            tgt_indices.append(next_token)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "        \n",
    "        # Decode to text\n",
    "        translation = vocab_tgt.decode(tgt_indices)\n",
    "        \n",
    "        return translation\n",
    "\n",
    "\n",
    "def translate_batch(model, sentences, vocab_src, vocab_tgt, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        sentences: List of source sentences\n",
    "        vocab_src: Source vocabulary\n",
    "        vocab_tgt: Target vocabulary\n",
    "        device: Device to run on\n",
    "        max_length: Maximum translation length\n",
    "    \n",
    "    Returns:\n",
    "        List of translated sentences\n",
    "    \"\"\"\n",
    "    translations = []\n",
    "    \n",
    "    for sentence in tqdm(sentences, desc=\"Translating\"):\n",
    "        translation = translate_sentence(model, sentence, vocab_src, vocab_tgt, device, max_length)\n",
    "        translations.append(translation)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "print(\" Translation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba3c8",
   "metadata": {},
   "source": [
    "## 9. Main Training Execution\n",
    "\n",
    "This is the main block where the training process is executed.\n",
    "\n",
    "-   **Optimizer and Scheduler**: An `Adam` optimizer and a learning rate scheduler (`ReduceLROnPlateau`) are initialized.\n",
    "-   **Training Loop**: The code iterates through the specified number of epochs, calling `train_fn` and `eval_fn` at each step.\n",
    "-   **Checkpointing**: The model with the best validation loss is saved to a file, allowing for inference or resuming training later.\n",
    "-   **History Tracking**: Training and validation losses are stored in a history dictionary for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f61da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ter(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute Translation Error Rate (TER) using edit distance.\n",
    "    Simplified version - production code would use specialized TER implementation.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference translation (string)\n",
    "        hypothesis: Hypothesis translation (string)\n",
    "    \n",
    "    Returns:\n",
    "        TER score (lower is better)\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    # Compute edit distance (Levenshtein distance)\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    \n",
    "    edit_distance = dp[m][n]\n",
    "    ter = edit_distance / max(len(ref_tokens), 1)\n",
    "    \n",
    "    return ter\n",
    "\n",
    "\n",
    "def compute_all_metrics(references, hypotheses, max_n=4):\n",
    "    \"\"\"\n",
    "    Compute comprehensive translation metrics including BLEU, METEOR, and TER.\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference translations (strings)\n",
    "        hypotheses: List of predicted translations (strings)\n",
    "        max_n: Maximum n-gram for BLEU (default: 4)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all computed metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Tokenize for BLEU/METEOR\n",
    "    ref_tokens = [[ref.split()] for ref in references]\n",
    "    hyp_tokens = [hyp.split() for hyp in hypotheses]\n",
    "    \n",
    "    # Smoothing function for BLEU\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Compute BLEU scores for different n-grams\n",
    "    for n in range(1, max_n + 1):\n",
    "        weights = [1.0/n] * n + [0.0] * (4 - n)\n",
    "        \n",
    "        scores = []\n",
    "        for ref, hyp in zip(ref_tokens, hyp_tokens):\n",
    "            score = sentence_bleu(ref, hyp, weights=weights, smoothing_function=smoothing)\n",
    "            scores.append(score)\n",
    "        \n",
    "        metrics[f'BLEU-{n}'] = np.mean(scores)\n",
    "        metrics[f'BLEU-{n}_std'] = np.std(scores)\n",
    "    \n",
    "    # Corpus-level BLEU-4\n",
    "    corpus_bleu_score = corpus_bleu(ref_tokens, hyp_tokens, smoothing_function=smoothing)\n",
    "    metrics['Corpus-BLEU-4'] = corpus_bleu_score\n",
    "    \n",
    "    # METEOR score\n",
    "    try:\n",
    "        meteor_scores = []\n",
    "        for ref, hyp in zip(references, hypotheses):\n",
    "            # METEOR expects tokenized inputs\n",
    "            score = meteor_score([ref.split()], hyp.split())\n",
    "            meteor_scores.append(score)\n",
    "        \n",
    "        metrics['METEOR'] = np.mean(meteor_scores)\n",
    "        metrics['METEOR_std'] = np.std(meteor_scores)\n",
    "    except Exception as e:\n",
    "        print(f\"  WARNING: Could not compute METEOR score: {e}\")\n",
    "        metrics['METEOR'] = None\n",
    "        metrics['METEOR_std'] = None\n",
    "    \n",
    "    # TER (Translation Error Rate)\n",
    "    ter_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ter = compute_ter(ref, hyp)\n",
    "        ter_scores.append(ter)\n",
    "    \n",
    "    metrics['TER'] = np.mean(ter_scores)\n",
    "    metrics['TER_std'] = np.std(ter_scores)\n",
    "    \n",
    "    # Additional statistics\n",
    "    metrics['num_samples'] = len(references)\n",
    "    metrics['avg_ref_length'] = np.mean([len(ref.split()) for ref in references])\n",
    "    metrics['avg_hyp_length'] = np.mean([len(hyp.split()) for hyp in hypotheses])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_pairs, vocab_src, vocab_tgt, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and compute comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_pairs: List of (source, target) sentence pairs\n",
    "        vocab_src: Source vocabulary\n",
    "        vocab_tgt: Target vocabulary\n",
    "        device: Device to run on\n",
    "        num_samples: Number of samples to evaluate (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics and sample translations\n",
    "    \"\"\"\n",
    "    # Load best model if available\n",
    "    best_model_path = os.path.join(config['checkpoint_dir'], 'best_model_simple.pt')\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Loading best model for evaluation...\")\n",
    "        load_checkpoint(best_model_path, model, strict=False)\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if num_samples:\n",
    "        test_pairs = test_pairs[:num_samples]\n",
    "    \n",
    "    source_sentences = [src for src, _ in test_pairs]\n",
    "    reference_translations = [tgt for _, tgt in test_pairs]\n",
    "    \n",
    "    # Generate translations\n",
    "    print(f\"Translating {len(source_sentences):,} test sentences...\")\n",
    "    print(f\"This may take several minutes...\\n\")\n",
    "    \n",
    "    predicted_translations = translate_batch(model, source_sentences, vocab_src, vocab_tgt, device)\n",
    "    \n",
    "    # Compute all metrics\n",
    "    print(f\"Computing evaluation metrics...\")\n",
    "    metrics = compute_all_metrics(reference_translations, predicted_translations)\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'source': source_sentences,\n",
    "        'reference': reference_translations,\n",
    "        'predicted': predicted_translations\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\" compute_ter(): Translation Error Rate computation\")\n",
    "print(\" compute_all_metrics(): BLEU, METEOR, TER\")\n",
    "print(\" evaluate_model(): Comprehensive evaluation with all metrics\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ede61d",
   "metadata": {},
   "source": [
    "## 10. Save Training History\n",
    "\n",
    "The training and validation loss history is saved to a JSON file for persistent storage and later analysis. This allows for plotting the learning curves without needing to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on FULL test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING EVALUATION ON FULL TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test set size: {len(test_pairs):,} pairs\")\n",
    "print(f\"This will take approximately {len(test_pairs) * 0.5 / 60:.1f}-{len(test_pairs) * 1.0 / 60:.1f} minutes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "evaluation_results = evaluate_model(\n",
    "    model, \n",
    "    test_pairs, \n",
    "    vocab_en, \n",
    "    vocab_ur, \n",
    "    device, \n",
    "    num_samples=None  # Evaluate on ALL test samples\n",
    ")\n",
    "\n",
    "# Display metrics in formatted table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION RESULTS - COMPREHENSIVE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics = evaluation_results['metrics']\n",
    "\n",
    "# Main metrics table\n",
    "print(\"\\n Primary Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<20} {'Score':>10} {'Std Dev':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# BLEU scores\n",
    "for n in range(1, 5):\n",
    "    score = metrics.get(f'BLEU-{n}', 0)\n",
    "    std = metrics.get(f'BLEU-{n}_std', 0)\n",
    "    print(f\"{'BLEU-' + str(n):<20} {score:>10.4f} {std:>10.4f}\")\n",
    "\n",
    "print(f\"{'Corpus-BLEU-4':<20} {metrics.get('Corpus-BLEU-4', 0):>10.4f} {'N/A':>10}\")\n",
    "\n",
    "# METEOR\n",
    "if metrics.get('METEOR') is not None:\n",
    "    print(f\"{'METEOR':<20} {metrics['METEOR']:>10.4f} {metrics.get('METEOR_std', 0):>10.4f}\")\n",
    "else:\n",
    "    print(f\"{'METEOR':<20} {'N/A':>10} {'N/A':>10}\")\n",
    "\n",
    "# TER\n",
    "print(f\"{'TER (lower=better)':<20} {metrics.get('TER', 0):>10.4f} {metrics.get('TER_std', 0):>10.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n Dataset Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Number of samples: {metrics.get('num_samples', 0):,}\")\n",
    "print(f\"Avg reference length: {metrics.get('avg_ref_length', 0):.2f} words\")\n",
    "print(f\"Avg hypothesis length: {metrics.get('avg_hyp_length', 0):.2f} words\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Save results to JSON\n",
    "results_to_save = {\n",
    "    'metrics': {k: float(v) if v is not None and not isinstance(v, str) else v \n",
    "                for k, v in metrics.items()},\n",
    "    'model_config': config,\n",
    "    'dataset_info': {\n",
    "        'total_pairs': len(parallel_data),\n",
    "        'train_pairs': len(train_pairs),\n",
    "        'val_pairs': len(val_pairs),\n",
    "        'test_pairs': len(test_pairs)\n",
    "    },\n",
    "    'training_info': {\n",
    "        'train_losses': train_losses if 'train_losses' in locals() else [],\n",
    "        'val_losses': val_losses if 'val_losses' in locals() else [],\n",
    "        'total_epochs': len(train_losses) if 'train_losses' in locals() else 0,\n",
    "        'best_val_loss': min(val_losses) if 'val_losses' in locals() and val_losses else None\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = os.path.join('results', 'evaluation_results.json')\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Evaluation results saved to: {results_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db7545",
   "metadata": {},
   "source": [
    "## 11. Plotting Learning Curves\n",
    "\n",
    "This section visualizes the model's training progress by plotting the training and validation losses over epochs.\n",
    "\n",
    "-   **Load History**: The saved training history is loaded from the JSON file.\n",
    "-   **Plot Losses**: The losses are plotted using `matplotlib` to create a learning curve, which helps in diagnosing issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a42618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample translations with detailed metrics\n",
    "num_samples_to_show = min(15, len(evaluation_results['source']))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE TRANSLATIONS WITH QUALITY SCORES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute metrics for each sample\n",
    "sample_metrics = []\n",
    "for i in range(num_samples_to_show):\n",
    "    src = evaluation_results['source'][i]\n",
    "    ref = evaluation_results['reference'][i]\n",
    "    pred = evaluation_results['predicted'][i]\n",
    "    \n",
    "    # Compute individual metrics\n",
    "    bleu = sentence_bleu([ref.split()], pred.split(), \n",
    "                        smoothing_function=SmoothingFunction().method1)\n",
    "    ter = compute_ter(ref, pred)\n",
    "    \n",
    "    # Length info\n",
    "    src_len = len(src.split())\n",
    "    ref_len = len(ref.split())\n",
    "    pred_len = len(pred.split())\n",
    "    \n",
    "    sample_metrics.append({\n",
    "        'index': i,\n",
    "        'src': src,\n",
    "        'ref': ref,\n",
    "        'pred': pred,\n",
    "        'bleu': bleu,\n",
    "        'ter': ter,\n",
    "        'src_len': src_len,\n",
    "        'ref_len': ref_len,\n",
    "        'pred_len': pred_len\n",
    "    })\n",
    "\n",
    "# Sort by BLEU score to show best and worst\n",
    "sample_metrics_sorted = sorted(sample_metrics, key=lambda x: x['bleu'], reverse=True)\n",
    "\n",
    "# Show top 5 best\n",
    "print(\"\\n TOP 5 BEST TRANSLATIONS (by BLEU score):\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(sample_metrics_sorted[:5], 1):\n",
    "    print(f\"\\n{i}. Original Index: {sample['index'] + 1}\")\n",
    "    print(f\"   Source (EN):     {sample['src']}\")\n",
    "    print(f\"   Reference (UR):  {sample['ref']}\")\n",
    "    print(f\"   Predicted (UR):  {sample['pred']}\")\n",
    "    print(f\"    BLEU: {sample['bleu']:.4f} | TER: {sample['ter']:.4f} | Lengths: {sample['src_len']}→{sample['pred_len']} (ref: {sample['ref_len']})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Show bottom 5 worst\n",
    "print(\"\\n  BOTTOM 5 WORST TRANSLATIONS (by BLEU score):\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(sample_metrics_sorted[-5:], 1):\n",
    "    print(f\"\\n{i}. Original Index: {sample['index'] + 1}\")\n",
    "    print(f\"   Source (EN):     {sample['src']}\")\n",
    "    print(f\"   Reference (UR):  {sample['ref']}\")\n",
    "    print(f\"   Predicted (UR):  {sample['pred']}\")\n",
    "    print(f\"    BLEU: {sample['bleu']:.4f} | TER: {sample['ter']:.4f} | Lengths: {sample['src_len']}→{sample['pred_len']} (ref: {sample['ref_len']})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Show some random samples\n",
    "print(\"\\n RANDOM SAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "import random as rand\n",
    "random_samples = rand.sample(sample_metrics, min(5, len(sample_metrics)))\n",
    "for i, sample in enumerate(random_samples, 1):\n",
    "    print(f\"\\n{i}. Original Index: {sample['index'] + 1}\")\n",
    "    print(f\"   Source (EN):     {sample['src']}\")\n",
    "    print(f\"   Reference (UR):  {sample['ref']}\")\n",
    "    print(f\"   Predicted (UR):  {sample['pred']}\")\n",
    "    print(f\"    BLEU: {sample['bleu']:.4f} | TER: {sample['ter']:.4f} | Lengths: {sample['src_len']}→{sample['pred_len']} (ref: {sample['ref_len']})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Analysis of common patterns\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANSLATION QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BLEU distribution\n",
    "bleu_scores = [s['bleu'] for s in sample_metrics]\n",
    "print(f\"\\nBLEU Score Distribution:\")\n",
    "print(f\"  Mean: {np.mean(bleu_scores):.4f}\")\n",
    "print(f\"  Median: {np.median(bleu_scores):.4f}\")\n",
    "print(f\"  Std Dev: {np.std(bleu_scores):.4f}\")\n",
    "print(f\"  Min: {min(bleu_scores):.4f}\")\n",
    "print(f\"  Max: {max(bleu_scores):.4f}\")\n",
    "\n",
    "# Length analysis\n",
    "length_ratios = [s['pred_len'] / max(s['ref_len'], 1) for s in sample_metrics]\n",
    "print(f\"\\nLength Ratio (Predicted/Reference):\")\n",
    "print(f\"  Mean: {np.mean(length_ratios):.4f}\")\n",
    "print(f\"  Median: {np.median(length_ratios):.4f}\")\n",
    "\n",
    "# Quality categories\n",
    "excellent = sum(1 for s in bleu_scores if s >= 0.7)\n",
    "good = sum(1 for s in bleu_scores if 0.4 <= s < 0.7)\n",
    "fair = sum(1 for s in bleu_scores if 0.2 <= s < 0.4)\n",
    "poor = sum(1 for s in bleu_scores if s < 0.2)\n",
    "\n",
    "print(f\"\\nQuality Distribution:\")\n",
    "print(f\"  Excellent (≥0.7): {excellent:3d} ({excellent/len(bleu_scores)*100:5.1f}%)\")\n",
    "print(f\"  Good (0.4-0.7):   {good:3d} ({good/len(bleu_scores)*100:5.1f}%)\")\n",
    "print(f\"  Fair (0.2-0.4):   {fair:3d} ({fair/len(bleu_scores)*100:5.1f}%)\")\n",
    "print(f\"  Poor (<0.2):      {poor:3d} ({poor/len(bleu_scores)*100:5.1f}%)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a03262",
   "metadata": {},
   "source": [
    "## 12. Inference and Translation\n",
    "\n",
    "This section defines the `translate_sentence` function, which uses the trained model to translate a new English sentence into Urdu.\n",
    "\n",
    "-   **Load Best Model**: The best-performing model checkpoint is loaded.\n",
    "-   **Translation Logic**: The function tokenizes the input sentence, generates a translation token by token using a greedy decoding approach, and detokenizes the output back into a human-readable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23060455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "if 'train_losses' in locals() and 'val_losses' in locals() and len(train_losses) > 0:\n",
    "    print(\"\\nGenerating training curves...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Plot both losses on same figure\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', \n",
    "             markersize=4, markevery=max(1, len(epochs)//20))\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s',\n",
    "             markersize=4, markevery=max(1, len(epochs)//20))\n",
    "    \n",
    "    # Find and mark best epoch\n",
    "    best_epoch = np.argmin(val_losses) + 1\n",
    "    best_val_loss = min(val_losses)\n",
    "    plt.plot(best_epoch, best_val_loss, 'g*', markersize=20, \n",
    "             label=f'Best (Epoch {best_epoch}: {best_val_loss:.4f})')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss Over Epochs', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.legend(fontsize=12, loc='best', framealpha=0.9)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Add min/max annotations\n",
    "    plt.text(0.02, 0.98, \n",
    "             f'Min Train Loss: {min(train_losses):.4f}\\nMin Val Loss: {best_val_loss:.4f}',\n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot with high DPI\n",
    "    plot_path = os.path.join('results', 'training_loss.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\" Training curves saved to: {plot_path}\")\n",
    "    \n",
    "    # Also create a separate plot for loss dynamics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left plot: Loss curves\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right plot: Loss difference (overfitting indicator)\n",
    "    loss_diff = [val - train for val, train in zip(val_losses, train_losses)]\n",
    "    ax2.plot(epochs, loss_diff, 'purple', linewidth=2, marker='o', markersize=3)\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Val Loss - Train Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Overfitting Indicator', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.fill_between(epochs, 0, loss_diff, where=[d > 0 for d in loss_diff], \n",
    "                     alpha=0.3, color='red', label='Overfitting')\n",
    "    ax2.fill_between(epochs, 0, loss_diff, where=[d <= 0 for d in loss_diff], \n",
    "                     alpha=0.3, color='green', label='Underfitting')\n",
    "    ax2.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    detailed_plot_path = os.path.join('results', 'training_analysis.png')\n",
    "    plt.savefig(detailed_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\" Detailed analysis saved to: {detailed_plot_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total epochs trained: {len(train_losses)}\")\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Loss improvement: {val_losses[0]:.4f} → {best_val_loss:.4f} ({(val_losses[0] - best_val_loss)/val_losses[0]*100:.1f}%)\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    final_diff = val_losses[-1] - train_losses[-1]\n",
    "    if final_diff > 0.5:\n",
    "        print(f\"\\n  WARNING: Possible overfitting detected (val-train diff: {final_diff:.4f})\")\n",
    "    elif final_diff < -0.1:\n",
    "        print(f\"\\n  WARNING: Possible underfitting (val-train diff: {final_diff:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n Good fit (val-train diff: {final_diff:.4f})\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n  No training history available to plot.\")\n",
    "    print(\"   Train the model first to generate loss curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa77f2",
   "metadata": {},
   "source": [
    "## 13. BLEU Score Evaluation\n",
    "\n",
    "This section evaluates the quality of the translations using the BLEU score.\n",
    "\n",
    "-   **`compute_bleu`**: A function that calculates BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores by comparing the model's translations to reference translations.\n",
    "-   **Evaluation**: The function is called with sample translated and reference sentences to compute the BLEU scores, providing a quantitative measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive translation\n",
    "# Change this to any English sentence you want to translate\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Thank you very much.\",\n",
    "    \"How old are you?\",\n",
    "    \"Where are you from?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INTERACTIVE TRANSLATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    translation = translate_sentence(model, sentence, vocab_en, vocab_ur, device)\n",
    "    print(f\"\\n{i}. English:  {sentence}\")\n",
    "    print(f\"   Urdu:     {translation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nTo translate your own sentences:\")\n",
    "print(\"1. Modify the 'test_sentences' list above\")\n",
    "print(\"2. Run this cell again\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
