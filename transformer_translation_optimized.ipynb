{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f47d79",
   "metadata": {},
   "source": [
    "# Project: English-to-Urdu Machine Translation\n",
    "\n",
    "This notebook presents a complete pipeline for fine-tuning a Transformer-based model for English-to-Urdu translation. The implementation is heavily optimized for modern Apple Silicon (M-series chips) using PyTorch's Metal Performance Shaders (MPS) backend.\n",
    "\n",
    "The primary goal is to demonstrate an effective and efficient training process that fulfills the core requirements of building, training, and evaluating a sequence-to-sequence model.\n",
    "\n",
    "### Key Features & Optimizations\n",
    "\n",
    "1.  **Fine-Tuning a Pre-trained Model**: Leverages the `Helsinki-NLP/opus-mt-en-ur` model, a powerful pre-trained baseline, to achieve high-quality translations with minimal training time.\n",
    "\n",
    "2.  **MPS-Optimized Training**: The entire workflow is tailored for Apple Silicon GPUs.\n",
    "    -   **Device Placement**: All tensors and model components are explicitly placed on the `mps` device.\n",
    "    -   **Mixed-Precision Training (AMP)**: Uses `torch.amp.autocast(device_type=\"mps\")` to accelerate computation and reduce memory usage by performing operations in `float16`.\n",
    "\n",
    "3.  **Efficient Data Pipeline**:\n",
    "    -   **Pre-tokenization**: The dataset is tokenized once and cached, eliminating this bottleneck from the training loop.\n",
    "    -   **Asynchronous Data Loading**: The `DataLoader` is configured with multiple workers (`num_workers`) and batch prefetching (`prefetch_factor`) to ensure the GPU is always saturated with data.\n",
    "\n",
    "4.  **Memory Efficiency**:\n",
    "    -   **Gradient Checkpointing**: Enabled to significantly reduce the memory footprint of the model, allowing for larger batch sizes.\n",
    "\n",
    "5.  **Custom Training Loop**: A transparent, from-scratch training loop is implemented to provide full control over the training process and integrate MPS-specific optimizations, replacing the high-level Hugging Face `Trainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feac6db",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    get_scheduler,\n",
    ")\n",
    "from datasets import Dataset as HFDataset, load_dataset\n",
    "\n",
    "# Evaluation metrics\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# MPS-ONLY OPTIMIZATIONS\n",
    "# ============================================================\n",
    "torch.set_default_device(\"mps\")\n",
    "# Enable fusion to optimize kernel performance (requires PyTorch 2.0+)\n",
    "# torch.backends.mps.enable_fusion(True)\n",
    "device = torch.device(\"mps\")\n",
    "print(\"MPS set as default device and fusion enabled.\")\n",
    "\n",
    "\n",
    "# Create necessary directories\n",
    "Path(\"checkpoints_optimized\").mkdir(exist_ok=True)\n",
    "Path(\"data\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302db489",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Model\n",
    "    'model_name': \"Helsinki-NLP/opus-mt-en-ur\",\n",
    "    \n",
    "    # Training \n",
    "    'batch_size': 32,          # Increased batch size due to memory optimizations\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # Data Loader (MPS Optimized)\n",
    "    'num_workers': 4,          # Use multiple workers to prefetch data\n",
    "    'prefetch_factor': 2,      # Number of batches to prefetch per worker\n",
    "    'pin_memory': True,        # Speeds up host-to-device transfer\n",
    "    \n",
    "    # Sequences\n",
    "    'max_source_length': 128,\n",
    "    'max_target_length': 128,\n",
    "    \n",
    "    # Checkpointing & Logging\n",
    "    'checkpoint_dir': './checkpoints_optimized',\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d38f96",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing\n",
    "\n",
    "**Optimization**:\n",
    "- **Load from text files**: Directly load the parallel corpus.\n",
    "- **Pre-tokenization**: Tokenize the entire dataset once using `map` with `batched=True`. This is much faster than tokenizing on-the-fly.\n",
    "- **Caching**: The `datasets` library automatically caches the processed data, so subsequent runs are instantaneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(en_file, ur_file, tokenizer, max_samples=None):\n",
    "    \"\"\"Loads parallel data, converts to HF Dataset, and tokenizes.\"\"\"\n",
    "    pairs = []\n",
    "    with open(en_file, 'r', encoding='utf-8') as f_en, \\\n",
    "         open(ur_file, 'r', encoding='utf-8') as f_ur:\n",
    "        for i, (en_line, ur_line) in enumerate(zip(f_en, f_ur)):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            en = en_line.strip()\n",
    "            ur = ur_line.strip()\n",
    "            if en and ur:\n",
    "                pairs.append({'english': en, 'urdu': ur})\n",
    "    \n",
    "    print(f\"Loaded {len(pairs):,} sentence pairs.\")\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    raw_dataset = HFDataset.from_pandas(pd.DataFrame(pairs))\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenize English-Urdu pairs.\"\"\"\n",
    "        inputs = tokenizer(\n",
    "            examples['english'],\n",
    "            max_length=config['max_source_length'],\n",
    "            truncation=True,\n",
    "            padding='max_length' # Pad to max_length for consistent tensor shapes\n",
    "        )\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples['urdu'],\n",
    "                max_length=config['max_target_length'],\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "        return inputs\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = raw_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=['english', 'urdu'],\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = train_test_split['train']\n",
    "    val_dataset = train_test_split['test']\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset):,}\")\n",
    "    print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "# Load and process data\n",
    "DATA_DIR = Path(\"data\")\n",
    "train_dataset, val_dataset = load_and_preprocess_data(\n",
    "    en_file=DATA_DIR / \"english-corpus.txt\",\n",
    "    ur_file=DATA_DIR / \"urdu-corpus.txt\",\n",
    "    tokenizer=tokenizer,\n",
    "    # max_samples=20000  # Using a subset for faster demonstration\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7d2da",
   "metadata": {},
   "source": [
    "## 4. Model & DataLoaders\n",
    "\n",
    "**Optimization Details**:\n",
    "- **Pre-trained Transformer**: We use a complete sequence-to-sequence model from Hugging Face (`Helsinki-NLP/opus-mt-en-ur`), which includes the encoder-decoder Transformer architecture.\n",
    "- **Gradient Checkpointing**: `model.gradient_checkpointing_enable()` is called to reduce memory usage. This technique trades a small amount of computation time during the backward pass for a significant reduction in VRAM, allowing for larger batch sizes.\n",
    "- **Optimized `DataLoader`**:\n",
    "    - `num_workers > 0` enables multi-process data loading to prevent the CPU from being a bottleneck.\n",
    "    - `pin_memory=True` allows for faster data transfer from CPU to the MPS device.\n",
    "    - `prefetch_factor` works with `num_workers` to load several batches in advance.\n",
    "    - `generator` is explicitly set to the `mps` device to ensure compatibility with multi-worker data loading on Apple Silicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config['model_name']).to(device)\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# The model is already on the 'mps' device thanks to torch.set_default_device()\n",
    "print(f\"Model loaded on {model.device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create a generator for the MPS device to be used by the DataLoader\n",
    "g = torch.Generator(device='mps')\n",
    "\n",
    "# Create efficient DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=config['pin_memory'],\n",
    "    prefetch_factor=config['prefetch_factor'],\n",
    "    generator=g  # Pass the MPS generator to the DataLoader\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=config['pin_memory'],\n",
    "    prefetch_factor=config['prefetch_factor']\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with {config['num_workers']} workers and an MPS-compatible generator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110669",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "**Optimization Details**:\n",
    "- **Custom Loop**: A manual training loop provides full control and transparency, avoiding the overhead of the high-level `Trainer` class and allowing for direct integration of MPS-specific features.\n",
    "- **Mixed-Precision (AMP) for MPS**:\n",
    "    - The `with torch.amp.autocast(device_type=\"mps\", dtype=torch.float16):` context manager automatically casts model operations to a faster, lower-precision format (`float16`) where possible.\n",
    "    - Unlike CUDA, a `GradScaler` is not required for stable training with `autocast` on MPS.\n",
    "- **Fused Optimizer**: `AdamW(fused=True)` is used where available. This can merge multiple optimizer steps into a single kernel, improving performance on compatible hardware.\n",
    "- **Learning Rate Scheduler**: A linear scheduler dynamically adjusts the learning rate during training, which can lead to better and more stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "try:\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'], fused=True)\n",
    "    print(\"Using fused AdamW optimizer.\")\n",
    "except:\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    print(\"Fused AdamW not available. Using standard AdamW.\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = config['num_epochs'] * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "print(\"Starting training...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed-precision forward pass for MPS\n",
    "        with torch.amp.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Standard backward pass (no scaler needed for MPS autocast)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "total_duration = time.time() - total_start_time\n",
    "print(f\"\\nTraining complete in {total_duration/60:.2f} minutes.\")\n",
    "print(f\"Average time per epoch: {total_duration / config['num_epochs']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6dba6",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Translation\n",
    "\n",
    "**Optimization**:\n",
    "- **`torch.no_grad()`**: Disables gradient calculation, which reduces memory consumption and speeds up inference.\n",
    "- **Batched Evaluation**: The validation loop processes data in batches, which is much more efficient than one-by-one evaluation.\n",
    "- **Minimal CPU Transfer**: Data is kept on the GPU as long as possible. `.cpu()` is only called at the very end when decoding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            \n",
    "            generated_tokens = model.generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=config['max_target_length'],\n",
    "            ).cpu().numpy() # Move to CPU after generation for decoding\n",
    "\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Replace -100 in labels used for padding\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend([pred.strip() for pred in decoded_preds])\n",
    "            all_labels.extend([[label.strip()] for label in decoded_labels])\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    return bleu_score['score']\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "eval_start_time = time.time()\n",
    "bleu_result = evaluate_model(model, val_dataloader)\n",
    "eval_duration = time.time() - eval_start_time\n",
    "\n",
    "print(f\"Evaluation complete in {eval_duration:.2f}s\")\n",
    "print(f\"   BLEU Score: {bleu_result:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d959646",
   "metadata": {},
   "source": [
    "## 7. Qualitative Examples & Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, tokenizer, max_length=128):\n",
    "    \"\"\"Translate a single sentence.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=config['max_source_length'])\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"I am learning to code.\",\n",
    "    \"This is a ability test.\",\n",
    "    \"I like to cook\",\n",
    "    \"I am going to school.\",\n",
    "    \"The weather is very nice today.\",\n",
    "    \"Please give me a glass of water.\", \n",
    "    \"What is your name?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRANSLATION EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_examples:\n",
    "    translation = translate(text, model, tokenizer)\n",
    "    print(f\"EN: {text}\")\n",
    "    print(f\"UR: {translation}\\n\")\n",
    "\n",
    "# --- Performance Benchmark ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "new_epoch_time_s = total_duration / config['num_epochs']\n",
    "\n",
    "print(f\"Optimized epoch time: {new_epoch_time_s:.2f} seconds\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
